1. Shannon 資訊理論的核心
	•	Claude E. Shannon 在 1948 年提出的資訊理論指出，訊息的「資訊量」主要和可取的可能狀態數量（即「不確定度」）有關。
	•	若某事件只有 1 種可能，資訊量是 0（因為毫無不確定度）。若事件可能性越多、越均勻，則資訊量越大。

    Shannon Entropy (熵) 範例
        •	給定一個離散機率分佈  $\{p_1, p_2, \dots, p_n\}$ ，訊息的熵（資訊量）定義為

    $H(X) = -\sum_{i=1}^n p_i \log_2 p_i.$

    •	這個 H(X) 代表平均需要幾個「bit」來編碼或表示該隨機變數的結果。

---

2. 「信息量」不是「數學式複雜度」
	•	雖然在數學領域有Kolmogorov complexity等概念，用來衡量「描述某個對象所需最短程式碼長度」，但在一般「資訊量」或「通訊量」的意義裡，並不是說：
	•	「公式寫得越多越複雜，所以信息量越大」。
	•	資訊量更關注**「發生可能性」與「不確定度減少量」**，不是幾條方程或公式的長度/難度。
    
---

3. 例子：猜數字
	•	若對方給你一個「0～1023之間的整數」，你需要多少資訊量才能確認它是哪個數？
	•	其不確定度總共有 1024 種可能 → 以二進位表示，需要  $\log_2(1024) = 10  bit$ 的資訊。
	•	這不取決於你列了多少條數學式，而在於「多少可能情況需要區分」。
    
---

4. 方程式與資訊量的關係？
	1.	確實：要精準描述一個系統，可能需要一些數學方程式。然而：
	•	你可以用 1 條很長的複雜方程式，或者 10 條較簡短的方程式——但這和「資訊量」未必直接掛鉤。
	•	很多方程式也可能只是重複或推導細節，並不真正增添「新的不確定度」。
	2.	訊息內容 vs. 表達形式：
	•	兩個人都在描述同一個機率分佈，一個用 1 行函數、一個用 5 行展開式，所表達的「隨機變數不確定度」是一樣的。
    
---

5. 小結
	•	「信息量」 在資訊科學語境是「減少多少不確定度、或有多少可能狀態」的度量，多用 Shannon Entropy 或其他度量方式。
	•	並不代表「能列出幾條、多複雜的數學式」。有時幾條簡明方程就能容納龐大資訊量，也可能列出一堆複雜式子卻只描述少量資訊。
	•	因此，若你想量化「信息量」，核心考量是**「有多少潛在狀態要被區分」**，而不是「寫了多少或多難的數學公式」。
